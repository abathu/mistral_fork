üöÄ Starting training job...
‚è±Ô∏è Training start time:
2025-06-28 01:19:59.708978
/home/s2678328/miniconda3/envs/mistral/bin/python
[*] Mercury :: Launching =>>> üöÄ üôà üöÄ
	=>> "This wind, it is not an ending..." (Robert Jordan - A Memory of Light)
usage: train.py [-h] --config CONFIG
                [--checkpoint_frequency CHECKPOINT_FREQUENCY]
                [--num_nodes NUM_NODES]
                [--training_arguments.adam_beta2 TRAINING_ARGUMENTS.ADAM_BETA2]
                [--training_arguments.prediction_loss_only TRAINING_ARGUMENTS.PREDICTION_LOSS_ONLY]
                [--model.id MODEL.ID]
                [--training_arguments.do_train TRAINING_ARGUMENTS.DO_TRAIN]
                [--training_arguments.max_grad_norm TRAINING_ARGUMENTS.MAX_GRAD_NORM]
                [--run_id RUN_ID] [--seed SEED]
                [--training_arguments.weight_decay TRAINING_ARGUMENTS.WEIGHT_DECAY]
                [--training_arguments.lr_scheduler_type TRAINING_ARGUMENTS.LR_SCHEDULER_TYPE]
                [--training_arguments.learning_rate TRAINING_ARGUMENTS.LEARNING_RATE]
                [--training_arguments.eval_steps TRAINING_ARGUMENTS.EVAL_STEPS]
                [--dataset.dataset_dir DATASET.DATASET_DIR]
                [--model.pretrained_tokenizer MODEL.PRETRAINED_TOKENIZER]
                [--online_eval.stride ONLINE_EVAL.STRIDE] [--wandb WANDB]
                [--dataset.name DATASET.NAME]
                [--training_arguments.max_steps TRAINING_ARGUMENTS.MAX_STEPS]
                [--world_size WORLD_SIZE] [--run_training RUN_TRAINING]
                [--online_eval.do_wikitext ONLINE_EVAL.DO_WIKITEXT]
                [--local_rank LOCAL_RANK]
                [--artifacts.run_dir ARTIFACTS.RUN_DIR]
                [--training_arguments.sharded_ddp TRAINING_ARGUMENTS.SHARDED_DDP]
                [--dataset.num_proc DATASET.NUM_PROC]
                [--training_arguments.adam_beta1 TRAINING_ARGUMENTS.ADAM_BETA1]
                [--model.passthrough_tokenizer MODEL.PASSTHROUGH_TOKENIZER]
                [--training_arguments.save_steps TRAINING_ARGUMENTS.SAVE_STEPS]
                [--resume RESUME]
                [--model.reorder_and_upcast_attn MODEL.REORDER_AND_UPCAST_ATTN]
                [--dataset.eval_num_proc DATASET.EVAL_NUM_PROC]
                [--nproc_per_node NPROC_PER_NODE] [--group GROUP]
                [--effective_bsz EFFECTIVE_BSZ]
                [--training_arguments.logging_dir TRAINING_ARGUMENTS.LOGGING_DIR]
                [--dataset.id DATASET.ID] [--num_gpus NUM_GPUS]
                [--training_arguments.per_device_train_batch_size TRAINING_ARGUMENTS.PER_DEVICE_TRAIN_BATCH_SIZE]
                [--training_arguments.dataloader_num_workers TRAINING_ARGUMENTS.DATALOADER_NUM_WORKERS]
                [--training_arguments.run_name TRAINING_ARGUMENTS.RUN_NAME]
                [--online_eval.do_lambada ONLINE_EVAL.DO_LAMBADA]
                [--model.initial_weights MODEL.INITIAL_WEIGHTS]
                [--log_level LOG_LEVEL]
                [--training_arguments.adam_epsilon TRAINING_ARGUMENTS.ADAM_EPSILON]
                [--artifacts.cache_dir ARTIFACTS.CACHE_DIR]
                [--training_arguments.fp16 TRAINING_ARGUMENTS.FP16]
                [--model.config_path MODEL.CONFIG_PATH]
                [--training_arguments.gradient_accumulation_steps TRAINING_ARGUMENTS.GRADIENT_ACCUMULATION_STEPS]
                [--training_arguments.fp16_backend TRAINING_ARGUMENTS.FP16_BACKEND]
                [--training_arguments.evaluation_strategy TRAINING_ARGUMENTS.EVALUATION_STRATEGY]
                [--model.scale_attn_by_inverse_layer_idx MODEL.SCALE_ATTN_BY_INVERSE_LAYER_IDX]
                [--training_arguments.seed TRAINING_ARGUMENTS.SEED]
                [--training_arguments.logging_steps TRAINING_ARGUMENTS.LOGGING_STEPS]
                [--use_gpu USE_GPU] [--nnodes NNODES]
                [--training_arguments.per_device_eval_batch_size TRAINING_ARGUMENTS.PER_DEVICE_EVAL_BATCH_SIZE]
                [--training_arguments.deepspeed TRAINING_ARGUMENTS.DEEPSPEED]
                [--run_final_eval RUN_FINAL_EVAL]
                [--training_arguments.output_dir TRAINING_ARGUMENTS.OUTPUT_DIR]
                [--training_arguments.logging_first_step TRAINING_ARGUMENTS.LOGGING_FIRST_STEP]
                [--dataset.validation_ratio DATASET.VALIDATION_RATIO]
                [--model.gradient_checkpointing MODEL.GRADIENT_CHECKPOINTING]
                [--training_arguments.ignore_data_skip TRAINING_ARGUMENTS.IGNORE_DATA_SKIP]
                [--model.seq_len MODEL.SEQ_LEN]
                [--training_arguments.gradient_checkpointing TRAINING_ARGUMENTS.GRADIENT_CHECKPOINTING]
                [--training_arguments.warmup_steps TRAINING_ARGUMENTS.WARMUP_STEPS]
                [--wandb_api_key_path WANDB_API_KEY_PATH]
                [--training_arguments.local_rank TRAINING_ARGUMENTS.LOCAL_RANK]
                [--resume_checkpoint RESUME_CHECKPOINT]
train.py: error: the following arguments are required: --config
bash: --config: command not found
‚è±Ô∏è Training end time:
2025-06-28 01:20:22.503311
‚úÖ Training finished
